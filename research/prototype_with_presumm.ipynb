{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a pre-trained summarization model, create one instance for every input, then decode from the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/nlpyang/PreSumm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updates: For encoding a text longer than 512 tokens, for example 800. Set max_pos to 800 during both preprocessing and training.\n",
    "\n",
    "-mode can be {validate, test}, where validate will inspect the model directory and evaluate the model for each newly saved checkpoint, test need to be used with -test_from, indicating the checkpoint you want to use\n",
    "MODEL_PATH is the directory of saved checkpoints\n",
    "use -mode valiadte with -test_all, the system will load all saved checkpoints and select the top ones to generate summaries (this will take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the baseline setup results in memory error, try building on MT-GPU, or containerize for ease of use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "\n",
    "# probably pytorch version in their requirements.txt\n",
    "# RuntimeError: cuda runtime error (38) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:51\n",
    "\n",
    "\n",
    "cd ~/projects/PreSumm/src\n",
    "\n",
    "source activate presumm\n",
    "\n",
    "BATCH_SIZE=1\n",
    "# note last part of BERT_DATA_PATH is file prefix\n",
    "BERT_DATA_PATH=/data/PreSumm_data/bert_data/bert_data_cnndm_final/cnndm\n",
    "MODEL_PATH=/data/PreSumm_data/models\n",
    "\n",
    "python train.py \\\n",
    "  -task abs \\\n",
    "  -mode validate \\\n",
    "  -batch_size ${BATCH_SIZE} \\\n",
    "  -test_batch_size ${BATCH_SIZE} \\\n",
    "  -bert_data_path ${BERT_DATA_PATH} \\\n",
    "  -log_file ../logs/val_abs_bert_cnndm \\\n",
    "  -model_path ${MODEL_PATH} \\\n",
    "  -sep_optim true \\\n",
    "  -use_interval true \\\n",
    "  -visible_gpus 0 \\\n",
    "  -max_pos 512 \\\n",
    "  -max_length 200 \\\n",
    "  -alpha 0.95 \\\n",
    "  -min_length 50 \\\n",
    "  -result_path ../logs/abs_bert_cnndm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CORENLP_HOME=/data/stanford_core_nlp/stanford-corenlp-full-2018-10-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "\n",
    "export CLASSPATH=/data/stanford_core\n",
    "\n",
    "\n",
    "java edu.stanford.nlp.pipeline.StanfordCoreNLP \\\n",
    "  -annotators tokenize,ssplit \\\n",
    "  -ssplit.newlineIsSentenceBreak always \\ \n",
    "  -filelist mapping_for_corenlp.txt \\\n",
    "  -outputFormat json \\\n",
    "  -outputDirectory tokenized_stories_dir\n",
    "\n",
    "\n",
    "command = ['java', 'edu.stanford.nlp.pipeline.StanfordCoreNLP', '-annotators', 'tokenize,ssplit',\n",
    "               '-ssplit.newlineIsSentenceBreak', 'always', '-filelist', 'mapping_for_corenlp.txt', '-outputFormat',\n",
    "               'json', '-outputDirectory', tokenized_stories_dir]\n",
    "    print(\"Tokenizing %i files in %s and saving in %s...\" % (len(stories), stories_dir, tokenized_stories_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "# NOTE: we still need to clean the multinews format (removing NEWLINE tokens and document separators, etc...)\n",
    "\n",
    "export CORENLP_HOME=/data/stanford_core_nlp/stanford-corenlp-full-2018-10-05\n",
    "\n",
    "# annotate -i val.src.100 -f json --annotators tokenize ssplit | jq '{src: [.[][] | [.tokens[].word]]}' > val.src.100.corenlp.json \n",
    "\n",
    "\n",
    "# WORKING one-liner\n",
    "jq -n \\\n",
    "  --slurpfile o1 <(annotate -i val.src.50 -f json --annotators tokenize ssplit | jq '{src: [.[][] | [.tokens[].word]]}') \\\n",
    "  --slurpfile o2 <(annotate -i val.tgt.50 -f json --annotators tokenize ssplit | jq '{tgt: [.[][] | [.tokens[].word]]}') \\\n",
    "  'reduce range(0; $o1|length) as $i ([]; . + [{ \"src\": $o1[$i].src, \"tgt\": $o2[$i].tgt}])' | less\n",
    "\n",
    "\n",
    "export CORENLP_HOME=/data/stanford_core_nlp/stanford-corenlp-full-2018-10-05\n",
    "DATADIR=/data/PreSumm_data/multi-news/preprocessed_truncated\n",
    "VALID_SRC=${DATADIR}/test.txt.src.tokenized.fixed.cleaned.final.truncated.txt\n",
    "VALID_TGT=${DATADIR}/test.txt.tgt.tokenized.fixed.cleaned.final.truncated.txt\n",
    "VALID_OUT=${DATADIR}/test.corenlp.json\n",
    "jq -n \\\n",
    "  --slurpfile o1 <(annotate -i ${VALID_SRC} -f json --annotators tokenize ssplit | jq '{src: [.[][] | [.tokens[].word]]}') \\\n",
    "  --slurpfile o2 <(annotate -i ${VALID_TGT} -f json --annotators tokenize ssplit | jq '{tgt: [.[][] | [.tokens[].word]]}') \\\n",
    "  'reduce range(0; $o1|length) as $i ([]; . + [{ \"src\": $o1[$i].src, \"tgt\": $o2[$i].tgt}])' > ${VALID_OUT}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# After the one-liner above we need to map into .pt files\n",
    "# Note file must have prefix in ['train', 'valid', 'test']\n",
    "\n",
    "source activate presumm\n",
    "PRESUM=/home/chrishokamp/projects/PreSumm\n",
    "JSON_DIR=/data/PreSumm_data/multi-news/preprocessed_truncated/presumm_json_input\n",
    "OUTPUT_DIR=${JSON_DIR}/bert_files_for_presumm\n",
    "mkdir -p ${OUTPUT_DIR}\n",
    "cd ${JSON_DIR}\n",
    "\n",
    "python $PRESUM/src/preprocess.py \\\n",
    " -mode format_to_bert \\\n",
    " -raw_path ${JSON_DIR} \\\n",
    " -save_path ${OUTPUT_DIR} \\\n",
    " -lower \\\n",
    " -n_cpus 1 \\\n",
    " -log_file preprocess.log\n",
    "\n",
    "\n",
    "# now rename files so that the prefixes work\n",
    "cp test.multinews.corenlp.bert.pt multinews.test.corenlp.bert.pt\n",
    "\n",
    "\n",
    "# Try summarizing the (flattened) multinews file\n",
    "# TODO: increase max length of summaries to fit with MultiNews dataset \n",
    "cd ~/projects/PreSumm/src\n",
    "\n",
    "source activate presumm\n",
    "\n",
    "BATCH_SIZE=32\n",
    "MAX_SUMMARY_LENGTH=128\n",
    "# note last part of BERT_DATA_PATH is file prefix\n",
    "BERT_DATA_PATH=/data/PreSumm_data/multi-news/preprocessed_truncated/presumm_json_input/bert_files_for_presumm/multinews\n",
    "MODEL_PATH=/data/PreSumm_data/models\n",
    "\n",
    "python train.py \\\n",
    "  -task abs \\\n",
    "  -mode validate \\\n",
    "  -batch_size ${BATCH_SIZE} \\\n",
    "  -test_batch_size ${BATCH_SIZE} \\\n",
    "  -bert_data_path ${BERT_DATA_PATH} \\\n",
    "  -log_file ../logs/val_abs_bert_cnndm \\\n",
    "  -model_path ${MODEL_PATH} \\\n",
    "  -sep_optim true \\\n",
    "  -use_interval true \\\n",
    "  -visible_gpus 0 \\\n",
    "  -max_pos 512 \\\n",
    "  -max_length ${MAX_SUMMARY_LENGTH} \\\n",
    "  -alpha 0.95 \\\n",
    "  -min_length 50 \\\n",
    "  -result_path ../logs/abs_bert_cnndm \n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinews has rouge from opennmt, presumably this is what they used \n",
    "# https://github.com/Alex-Fabbri/Multi-News/blob/3675e7c422ae3b4020617a324ac264f50333357d/code/OpenNMT-py-baselines/tools/test_rouge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split every multinews line into constituent story files\n",
    "\n",
    "# download Stanford NLP and set classpath accordingly\n",
    "\n",
    "# remember presumm does a lot of idiosyncratic things with the BERT special tokenss\n",
    "\n",
    "def multinews_to_presumm_json_format(multinews_file):\n",
    "    \"\"\"Simplest possible thing: just flatten a multinews row into a single document\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Alex-Fabbri/Multi-News\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing to prepare a new test dataset\n",
    "\n",
    "# Note we try to go around having to use their clunky preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Format MultiNews to .json format of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Map json-formatted data to pytorch tensors for BERT, store them in a file that we can use \n",
    "#   to get the summaries for the MultiNews dev+test sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
