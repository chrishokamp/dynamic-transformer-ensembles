{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "\n",
    "def read_duc_2004_(root_dir):\n",
    "    root_dir = pathlib.Path(root_dir)\n",
    "    docs_dir = root_dir / 'DUC2004_Summarization_Documents/duc2004_testdata/tasks1and2/duc2004_tasks1and2_docs/docs'\n",
    "    result_dir = root_dir / 'duc2004_results'\n",
    "\n",
    "    def get_duc_cluster_docs(cluster_id):\n",
    "        docs = []\n",
    "        cluster_path = docs_dir / f'd{cluster_id}t'\n",
    "        for fpath in cluster_path.iterdir():\n",
    "            with open(fpath) as f:\n",
    "                raw = f.read()\n",
    "            text = raw.split(\"<TEXT>\")[1].split(\"</TEXT>\")[0]\n",
    "            text = \" \".join(text.split())\n",
    "            doc = {\n",
    "                'fname': fpath.name,\n",
    "                'cluster_id': cluster_id,\n",
    "                'text': text\n",
    "            }\n",
    "            docs.append(doc)\n",
    "        docs = sorted(docs, key=lambda x: x['fname'])\n",
    "        return docs\n",
    "\n",
    "    cid_to_clusters = {}\n",
    "    # get reference (models) and peer (participant systems) summaries\n",
    "    for group in [\"models\", \"peers\"]:\n",
    "\n",
    "        gz_path = result_dir / f'ROUGE/duc2004.task2.ROUGE.{group}.tar.gz'\n",
    "        tar = tarfile.open(gz_path, \"r:gz\")\n",
    "        for member in tar.getmembers():\n",
    "\n",
    "            author_id = member.name.split(\".\")[-1]\n",
    "            cluster_id = member.name.split(\"/\")[-1].split(\".\")[0].lstrip(\"D\")\n",
    "\n",
    "            # print(member.name)\n",
    "            # print('CID:', cluster_id)\n",
    "            # print()\n",
    "\n",
    "            with tar.extractfile(member) as f:\n",
    "                text = str(f.read(), encoding=\"UTF-8\")\n",
    "            text = \" \".join(text.split())\n",
    "\n",
    "            summary_item = {\n",
    "                'author_id': author_id,\n",
    "                'text': text,\n",
    "                'cluster_id': cluster_id\n",
    "            }\n",
    "\n",
    "            if cluster_id not in cid_to_clusters:\n",
    "                cid_to_clusters[cluster_id] = {\n",
    "                    'peer_summaries': [],\n",
    "                    'ref_summaries': [],\n",
    "                    'id': cluster_id\n",
    "                }\n",
    "\n",
    "            if group == \"models\":\n",
    "                cid_to_clusters[cluster_id]['ref_summaries'].append(summary_item)\n",
    "            elif group == \"peers\":\n",
    "                cid_to_clusters[cluster_id]['peer_summaries'].append(summary_item)\n",
    "\n",
    "    # get source documents\n",
    "    clusters = []\n",
    "    for cid, c in cid_to_clusters.items():\n",
    "        docs = get_duc_cluster_docs(cid)\n",
    "        c['documents'] = docs\n",
    "        print('CLUSTER:', cid, len(c['documents']))\n",
    "        clusters.append(c)\n",
    "    clusters = sorted(clusters, key=lambda x: x['id'])\n",
    "    print('#clusters:', len(clusters))\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def read_duc_2004(path):\n",
    "    for c in read_duc_2004_(path):\n",
    "        docs = [d['text'] for d in c['documents']]\n",
    "        summaries = [s['text'] for s in c['ref_summaries']]\n",
    "        yield docs, summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER: 30001 10\n",
      "CLUSTER: 30002 10\n",
      "CLUSTER: 30003 10\n",
      "CLUSTER: 30005 10\n",
      "CLUSTER: 30006 10\n",
      "CLUSTER: 30007 10\n",
      "CLUSTER: 30008 10\n",
      "CLUSTER: 30010 10\n",
      "CLUSTER: 30011 10\n",
      "CLUSTER: 30015 10\n",
      "CLUSTER: 30017 10\n",
      "CLUSTER: 30020 10\n",
      "CLUSTER: 30022 10\n",
      "CLUSTER: 30024 10\n",
      "CLUSTER: 30026 10\n",
      "CLUSTER: 30027 10\n",
      "CLUSTER: 30028 10\n",
      "CLUSTER: 30029 10\n",
      "CLUSTER: 30031 10\n",
      "CLUSTER: 30033 10\n",
      "CLUSTER: 30034 10\n",
      "CLUSTER: 30036 10\n",
      "CLUSTER: 30037 10\n",
      "CLUSTER: 30038 10\n",
      "CLUSTER: 30040 10\n",
      "CLUSTER: 30042 10\n",
      "CLUSTER: 30044 10\n",
      "CLUSTER: 30045 10\n",
      "CLUSTER: 30046 10\n",
      "CLUSTER: 30047 10\n",
      "CLUSTER: 30048 10\n",
      "CLUSTER: 30049 10\n",
      "CLUSTER: 30050 10\n",
      "CLUSTER: 30051 10\n",
      "CLUSTER: 30053 10\n",
      "CLUSTER: 30055 10\n",
      "CLUSTER: 30056 10\n",
      "CLUSTER: 30059 10\n",
      "CLUSTER: 31001 10\n",
      "CLUSTER: 31008 10\n",
      "CLUSTER: 31009 10\n",
      "CLUSTER: 31013 10\n",
      "CLUSTER: 31022 10\n",
      "CLUSTER: 31026 10\n",
      "CLUSTER: 31031 10\n",
      "CLUSTER: 31032 10\n",
      "CLUSTER: 31033 10\n",
      "CLUSTER: 31038 10\n",
      "CLUSTER: 31043 10\n",
      "CLUSTER: 31050 10\n",
      "#clusters: 50\n",
      "[(10, 50)]\n",
      "Input stats:\n",
      "(588.272, 191996.402016, 438.17394036615184)\n",
      "Summary stats:\n",
      "(104.57, 28.145100000000003, 5.305195566612036)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATADIR = '/home/chris/projects/aylien/dynamic-ensembles/data/DUC2004'\n",
    "\n",
    "cluster_rows = []\n",
    "\n",
    "article_cnts = Counter()\n",
    "summary_lens = []\n",
    "source_lens = []\n",
    "\n",
    "# DUC only has a test set\n",
    "with open(Path(DATADIR) / ('DUC2004_test.jsonl'), 'w') as out:\n",
    "    for srcs, tgts in read_duc_2004(DATADIR):\n",
    "        articles = [{'title': '', 'text': t} for t in srcs]\n",
    "        out.write(f'{json.dumps({\"articles\": articles, \"summary\": tgts})}\\n')\n",
    "        article_cnts.update([len(articles)])\n",
    "        source_lens.extend([len(a.split()) for a in srcs])\n",
    "        summary_lens.extend([len(t.split()) for t in tgts])\n",
    "\n",
    "print(article_cnts.most_common())\n",
    "print('Input stats:')\n",
    "print((np.mean(source_lens), np.var(source_lens), np.std(source_lens)))\n",
    "print('Summary stats:')\n",
    "print((np.mean(summary_lens), np.var(summary_lens), np.std(summary_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
